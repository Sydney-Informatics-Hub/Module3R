---
title: "MLR - Part 2 (Classification)"
author: "Sydney Informatics Hub"
output: html_document
---

> **Note** Nothing about this model specification is specific to the
> diabetes dataset.

### Find which parameters will give the model its best accuracy

During hyperparameters tuning we are going to:

-   Try different values and measure their performance;
-   Find good values for these parameters;
-   Once the value(s) of the parameter(s) are determined, a model can be
    finalized by fitting the model to the entire training set.

#### `grid_random()`

You have a couple of options for how to choose which possible values for
the tuning parameters to try. One of these options is creating a random
grid of values. **Random grid search** is implemented with the
`grid_random()` function in tidymodels, taking a *sequence of
hyperparameter names to create the grid*. It also has a `size` parameter
that specifies the number of random combinations to create.

**Instructions**

-   Use the `grid_random()` function to create a random grid of values
    for the hyperparameters `mtry()`, `trees()` and `min_n()`;

The `mtry()` hyperparameter requires a pre-set range of values to test
since it cannot exceed the number of columns in our data. When we add
this to `grid_random()` we can pass `mtry()` into the `range_set()`
function and set a range for the hyperparameter with a numeric vector.

-   In the code below, set the range from 3 to 6. This is because we
    have 9 columns in `d_na` and we would like to test `mtry()` values
    somewhere in the middle between 1 and 9, trying to avoid values
    close to the ends;

-   You can then use the function `tune_grid()` to tune either a
    workflow or a model specification with a set of resampled data, such
    as the cross-validation we created. Grid search, combined with
    resampling, requires fitting a lot of models! These models don't
    depend on one another and can be run in parallel.

```{r tuning, exercise=TRUE, exercise.blanks = "___+", exercise.timelimit = 1000}
set.seed(123)

rf_grid <- ____(mtry() %>% range_set(c(____, ____)),
                       trees(),
                       min_n(),
                       size = ____)

#View grid
rf_grid

#Tune random forest model 
rf_tune_model <- ____(
  rf_model_diabetes,  #your model
  diabetes_rec,       #your recipe
  resamples = diabetes_folds, #your resampling
  grid = ____,
  control = grid_ctrl)

rf_tune_model
```

```{r tuning-solution}
set.seed(123)

rf_grid <- grid_random(mtry() %>% range_set(c(3, 6)),
                       trees(),
                       min_n(),
                       size = 10)

#View grid
rf_grid

#Tune random forest model 
rf_tune_model <- tune_grid(
  rf_model_diabetes, 
  diabetes_rec,       
  resamples = diabetes_folds,
  grid = rf_grid,
  control = grid_ctrl)

rf_tune_model
```

Great job! Let's now use `collect_metrics()` and `show_best()` functions
to:

-   extract the metrics calculated from the cross-validation performance
    across the different values of the parameters;
-   to see which model performed the best, in terms of the **"roc_auc"**
    metric.

```{r besttuning, exercise=TRUE, exercise.blanks = "___+"}
# extract all metrics from the cross-validation performance
rf_tune_model %>%
  ____()

# see which model performed the best
rf_tune_model %>%
  ____("roc_auc")
```

```{r besttuning-solution}
rf_tune_model %>%
  collect_metrics()

rf_tune_model %>%
  show_best("roc_auc")
```

#### `data.frame()`

We can also specify the values of the parameters to tune with a tuning
grid, entered as a **data frame**. It contains all the combinations of
parameters to be tested.

**Instructions** Use the `data.frame()` function to create a tuning grid
for the `mixture` and `penalty` hyperparameters, to use later for tuning
our regularized logistic regression model:

```{r dftuning, exercise=TRUE, exercise.blanks = "___+"}
rlr_grid <- ____(mixture = seq(0, 1, 0.1),
                       penalty = seq(0, 1, 0.1))
rlr_grid
```

```{r dftuning-solution}
rlr_grid <- data.frame(mixture = seq(0, 1, 0.1),
                       penalty = seq(0, 1, 0.1))
rlr_grid
```

Well done! You just created a data frame named "rlr_grid" with two
columns: *"mixture"* and *"penalty"*. You did this by using the `seq()`
function with the parameters `0`, `1`, and `0.1`. This will create a
sequence of numbers starting from 0, incrementing by 0.1, and ending
at 1. The resulting data frame `rlr_grid` has 11 rows, corresponding to
the 11 values in the "mixture" and "penalty" sequences.

Let's use `tune_grid()` again for hyperparameters tuning of the
`rlr_model_diabetes` logistic regression model, doing cross validation
for each row of the `rlr_grid` tuning grid. Let's then use the
`collect_metrics()` and `show_best()` functions to visualise our best
tuning results for the **"roc_auc"** metric:

```{r lrtuning, exercise=TRUE, exercise.blanks = "___+"}
set.seed(123)

# hyperparameters tuning
rlr_tune_model <- tune_grid(
  ____,  #your model
  diabetes_rec, #your recipe 
  resamples = diabetes_folds, #your resampling 
  grid = ____, #your grid
  control = grid_ctrl)

# collect all metrics
rlr_tune_model %>%
  ____

# show the best "roc_auc" results
rlr_tune_model %>%
  show_best(____)
```

```{r lrtuning-solution}
set.seed(123)

rlr_tune_model <- tune_grid(
  rlr_model_diabetes,  
  diabetes_rec,       
  resamples = diabetes_folds, 
  grid = rlr_grid,
  control = grid_ctrl)

rlr_tune_model %>%
  collect_metrics()

rlr_tune_model %>%
  show_best("roc_auc")
```

## Compare models and preprocessing steps

Tidymodels allows us to perform all of the above steps in a much faster
way with the **workflowsets** package.

**Instructions** - Use the `workflow_set()` function to combine your
`rlr_model_diabetes` and `rf_model_diabetes` models with your
`diabetes_rec` recipe:

```{r workflowset, exercise=TRUE, exercise.blanks = "___+"}
diabetes_wf_set <- ____(list(diabetes_rec),  #list of recipes
             list(rf_model_diabetes, ____), #list of models
             cross = TRUE) #all combinations of the preprocessors and models are used to create the workflows

# print the workflow id
diabetes_wf_set$wflow_id

# print the options for each id
diabetes_wf_set$option
```

```{r workflowset-solution}
diabetes_wf_set <- workflow_set(list(diabetes_rec),  
             list(rf_model_diabetes, rlr_model_diabetes), 
             cross = TRUE) 

diabetes_wf_set$wflow_id

diabetes_wf_set$option
```

Great! You have now combined your recipe and models within the
`diabetes_wf_set` object. The workflow ids created have been
automatically named *recipe_rand_forest* and *recipe_logistic_reg*. If
we have a look at the options, there are no grids specified for our
workflows.

Let's add our `rf_grid` and `rlr_grid` girds with the `option_add()`
function:

```{r workflowset_grid, exercise=TRUE, exercise.blanks = "___+"}
diabetes_wf_set <- diabetes_wf_set %>%
  option_add(grid=rf_grid, id="recipe_rand_forest") %>% #add the rf_grid
  option_add(grid=____, id="recipe_logistic_reg")       #add the rlr_grid

diabetes_wf_set$option
```

```{r workflowset_grid-solution}
diabetes_wf_set <- diabetes_wf_set %>%
  option_add(grid=rf_grid, id="recipe_rand_forest") %>%
  option_add(grid=rlr_grid, id="recipe_logistic_reg")

diabetes_wf_set$option
```

We can now call `tune_grid()`. Note that the workflow set object
`diabetes_wf_set` already includes your recipe, your models and your
grids, so you will only need to add your resampling object to the
`workflow_map()` function.

> The first argument in the `workflow_map()` function is a function name
> from the **tune** package (`tune_grid()`, `fit_resamples()`..)

```{r workflowmap, exercise=TRUE, exercise.blanks = "___+", exercise.timelimit = 1000}
set.seed(123)

diabetes_wf_set <- diabetes_wf_set %>%
                   ____("tune_grid",
                   resamples = ____, 
                   control = grid_ctrl,
                   verbose = TRUE) 
diabetes_wf_set
```

```{r workflowmap-solution}
set.seed(123)

diabetes_wf_set <- diabetes_wf_set %>%
                   workflow_map("tune_grid",
                   resamples = diabetes_folds,
                   control = grid_ctrl,
                   verbose = TRUE) 
diabetes_wf_set
```

The `results` column contains the results of each call to `tune_grid()`
for the workflows. From these results, we can get quick assessments of
how well these models classified the data using the `rank_results()` and
`autoplot()` functions.

**Instructions** Call the `rank_results()` and `autoplot()` functions on
your tuned workflows. You specifically want to have a look at the
`"roc_auc"` metric:

```{r autoplot, exercise=TRUE, exercise.blanks = "___+"}
#to get the rankings of the models (and their tuning parameter sub-models) as a data frame:
rank_results(diabetes_wf_set, rank_metric = ____)

#to plot the results
autoplot(____, metric = "roc_auc")
```

```{r autoplot-solution}
rank_results(diabetes_wf_set, rank_metric = "roc_auc")

autoplot(diabetes_wf_set, metric = "roc_auc")
```

## Extract best tuning results

The plot above shows the results for all tuning parameter combinations
for each model. We can use the `extract_workflow_set_result()` and
`select_best()` functions to extract the best tuning results.

**Instructions**

-   Extract the tuning results for the `"recipe_logistic_reg"` workflow;
-   Select the best results for the `**roc_auc**` metric.

```{r best, exercise=TRUE, exercise.blanks = "___+"}
best_results <- diabetes_wf_set %>%
  extract_workflow_set_result(____) %>%
  ____(metric="roc_auc")

best_results
```

```{r best-solution}
best_results <- diabetes_wf_set %>%
  extract_workflow_set_result("recipe_logistic_reg") %>%
  select_best(metric="roc_auc")

best_results
```

## Update and fit the workflow

The last step in hyperparameter tuning is to use `finalize_workflow()`
to add our optimal model to our workflow object, and apply the
`last_fit()` function to our workflow and our train/test split object.
This will automatically train the model specified by the workflow using
the training data, and produce evaluations based on the test set.

**Instructions**

-   Use the `extract_workflow()` function to extract the specifications
    of your best workflow (`recipe_logistic_reg`);
-   Update your workflow with your optimal model (`best_results`) using
    the `finalize_workflow()` function;
-   Fit to the training set and evaluate on the testing set using
    `last_fit()`.

```{r update, exercise=TRUE, exercise.blanks = "___+", exercise.timelimit=800}
set.seed(123)

final_diabetes_fit <- diabetes_wf_set %>%
  ____("recipe_logistic_reg") %>%
  finalize_workflow(____) %>%
  ____(diabetes_split)

final_diabetes_fit

final_diabetes_fit %>% collect_metrics()
```

```{r update-solution}
set.seed(123)

final_diabetes_fit <- diabetes_wf_set %>%
  extract_workflow("recipe_logistic_reg") %>%
  finalize_workflow(best_results) %>%
  last_fit(diabetes_split)

final_diabetes_fit

final_diabetes_fit %>% collect_metrics()
```

> Since we supplied the train/test object when we fit the workflow, the
> metrics are evaluated on the test set. Now when we use the
> `collect_metrics()` function (the same we used when tuning our
> parameters) to extract the performance of the final model (since
> `final_diabetes_fit` now consists of a single final model) applied to
> the test set.

### AUC: Area under the curve and Confusion Matrix

A good classifier will have high precision and high specificity,
minimizing both false positives and false negatives. In practice, and
with an imperfect classifier, you can tune a knob to say which of those
two you care more about. There will be some kind of a trade-off between
the two.

To capture this balance, we often use a **Reciever Operator
Characteristic (ROC) curve** that plots the *false positive* rate along
the x-axis and the *true positive* rate along the y-axis, for all
possible trade-offs. A line that is diagonal from the lower left corner
to the upper right corner represents a random guess at labelling each
example. The higher the line is in the upper left-hand corner, the
better the classifier in general. AUC computes the area under this
curve. For a perfect classifier, AUC = 1, for a random guess, AUC=0.5.
Objective: maximize.

![A Reciever Operator Characteristic (ROC) curve, from which the Area
Under the Curve (AUC) can be calculated](images/_CatArea.jpg){width="80%"}

A **confusion matrix** is a matrix that compares "the truth" to the
labels generated by your classifier. When we label a cat correctly, we
refer to this as a *true positive*. When we fail to label a cat as a
cat, this is called a *false negative*. However, if we label something
which is not a cat as a cat, this is called a *false positive*; and if
we correctly label something which is not a cat, as not a cat, then this
is a *true negative*. In our case, the confusion matrix will look like
this:

-   true positive **(TP)**: Diabetic correctly identified as diabetic
-   true negative **(TN)**: Healthy correctly identified as healthy
-   false positive **(FP)**: Healthy incorrectly identified as diabetic
-   false negative **(FN)**: Diabetic incorrectly identified as healthy

![](images/_CatConfusion.jpg){width="80%"} 

We can plot the ROC curve to visualize
test set performance of our model, and generate a confusion matrix.

*Note In R, factor levels are ordered alphabetically by default, which
means that "no" comes first before "yes" and is considered the level of
interest or positive case. Use the argument `event_level = "second"` to
alter this as needed.*

**Instructions**

-   Extract the prediction values from your final fit object with
    `collect_predictions()`;
-   Plot a ROC curve with the `roc_curve()` function;
-   Plot a confusion matrix with the `conf_mat()` function.

```{r roc, exercise=TRUE, exercise.blanks = "___+"}
#ROC curve
  collect_predictions(final_diabetes_fit) %>%
  ____(diabetes, event_level="second", .pred_pos) %>%  #specify which level of truth to consider as the "event"
                autoplot()

#confusion matrix dataframe
conf_matrix_rf <- final_diabetes_fit %>%
  collect_predictions() %>%
  ____(truth = diabetes, estimate = .pred_class) 

conf_matrix_rf

#confusion matrix plot
conf_matrix_rf %>%
  autoplot()
```

```{r roc-solution}
  collect_predictions(final_diabetes_fit) %>%
  roc_curve(diabetes, event_level="second", .pred_pos) %>%
                autoplot()

conf_matrix_rf <- final_diabetes_fit %>%
  collect_predictions() %>%
  conf_mat(truth = diabetes, estimate = .pred_class) 

conf_matrix_rf

conf_matrix_rf %>%
  autoplot()
```

## Variable importance

In order to visualize the variable importance scores of our  model, we will need to:

-   manually train our workflow object with the `fit()` function on the training data;
-   extract the trained model with the `pull_workflow_fit()` function;
-   pass the trained model to the `vip()` function.

```{r vip, exercise=TRUE, exercise.blanks = "___+", exercise.timelimit=800}
#extract the final workflow
final_workflow <- diabetes_wf_set %>%
  extract_workflow("recipe_logistic_reg") %>%
  finalize_workflow(best_results)

#fit on the training data
wf_fit <- final_workflow %>%
  ____(data = d_na_train)

#extract the trained model
wf_fit <- wf_fit %>% 
          ____()

#plot variable importance
____(wf_fit)
```

```{r vip-solution}
final_workflow <- diabetes_wf_set %>%
  extract_workflow("recipe_logistic_reg") %>%
  finalize_workflow(best_results)

wf_fit <- final_workflow %>%
  fit(data = d_na_train)

wf_fit <- wf_fit %>% 
          pull_workflow_fit()

vip(wf_fit)
```